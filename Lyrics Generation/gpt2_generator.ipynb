{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkz_d5yP34Lo",
        "outputId": "b952c724-c284-4363-9d3c-700cdaae4ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Collecting lyricsgenius\n",
            "  Downloading lyricsgenius-3.0.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading lyricsgenius-3.0.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lyricsgenius\n",
            "Successfully installed lyricsgenius-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas tqdm beautifulsoup4 scikit-learn lyricsgenius"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lyricsgenius\n",
        "import pandas as pd\n",
        "import re\n",
        "pd.set_option('display.max_rows', None)"
      ],
      "metadata": {
        "id": "JEv-rfh44Eu3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = \"./Radiohead_lyrics.csv\"  # Replace with your CSV file path\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Combine all lyrics for pretraining, retaining line breaks\n",
        "all_lyrics = df['Lyrics'].str.cat(sep='\\n\\n')\n",
        "\n",
        "# Save all artist's lyrics to a text file with UTF-8 encoding\n",
        "with open(\"artist_lyrics.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(all_lyrics)\n",
        "\n",
        "with open('artist_lyrics.txt', 'r') as file:\n",
        "    lyrics = file.read()\n",
        "\n",
        "import re\n",
        "\n",
        "# Remove phrases like \"You might also like\", \"Embed\", and any contributors number at the beginning\n",
        "cleaned_lyrics = re.sub(r'\\d+\\sContributors', '', lyrics)  # Removes \"87 Contributors\" or similar\n",
        "cleaned_lyrics = re.sub(r'You might also like.*?Embed', '', cleaned_lyrics, flags=re.DOTALL)  # Removes \"You might also like\" section\n",
        "\n",
        "# Save the cleaned lyrics back to the file\n",
        "with open(\"artist_lyrics_cleaned.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(cleaned_lyrics)\n",
        "\n",
        "# Extract and save specific song's lyrics\n",
        "specific_song_title = \"No Surprises\"  # Replace with the specific song title\n",
        "song_lyrics = df[df['Song'] == specific_song_title]['Lyrics'].values[0]\n",
        "with open(\"song_lyrics.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(song_lyrics)\n"
      ],
      "metadata": {
        "id": "nB89F4lv4QAg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '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'\n"
      ],
      "metadata": {
        "id": "UEhqNphy-a77"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "model_name = \"gpt2\"  # or \"EleutherAI/gpt-neo-125M\" for GPT-Neo\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Prepare dataset\n",
        "def load_dataset(file_path, tokenizer, block_size=128):\n",
        "    return TextDataset(\n",
        "        tokenizer=tokenizer,\n",
        "        file_path=file_path,\n",
        "        block_size=block_size\n",
        "    )\n",
        "\n",
        "artist_lyrics_path = \"artist_lyrics_cleaned.txt\"\n",
        "dataset = load_dataset(artist_lyrics_path, tokenizer)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-artist\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "CBBSmx954lMO",
        "outputId": "50793a16-6a7a-4c71-e07f-447727de85ec"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='183' max='183' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [183/183 00:31, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=183, training_loss=2.95794510971653, metrics={'train_runtime': 31.99, 'train_samples_per_second': 11.347, 'train_steps_per_second': 5.721, 'total_flos': 23712251904000.0, 'train_loss': 2.95794510971653, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the specific song's lyrics dataset\n",
        "song_lyrics_path = \"song_lyrics.txt\"\n",
        "song_dataset = load_dataset(song_lyrics_path, tokenizer)\n",
        "\n",
        "# Update training arguments for fine-tuning\n",
        "fine_tuning_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-song\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize Trainer for fine-tuning\n",
        "fine_tuner = Trainer(\n",
        "    model=model,\n",
        "    args=fine_tuning_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=song_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "fine_tuner.train()\n"
      ],
      "metadata": {
        "id": "3BOa71yO4uLs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "483b41fc-dbf4-474c-fe7d-a43c16d12a95"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3/3 00:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=3, training_loss=2.9166170756022134, metrics={'train_runtime': 6.4144, 'train_samples_per_second': 0.468, 'train_steps_per_second': 0.468, 'total_flos': 195969024000.0, 'train_loss': 2.9166170756022134, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "model.save_pretrained(\"./gpt2-song\")\n",
        "tokenizer.save_pretrained(\"./gpt2-song\")\n",
        "\n",
        "print(\"Model fine-tuned and saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOU9j8T8vODU",
        "outputId": "c5df532f-626a-46c8-e69f-80490778f996"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model fine-tuned and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "# Load the tokenizer from the base GPT-2 model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Save the tokenizer files to the ./gpt2-song directory\n",
        "tokenizer.save_pretrained(\"./gpt2-song\")\n",
        "\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# Load the model's config from the base GPT-2 model\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Save the config file to the ./gpt2-song directory\n",
        "model.config.to_json_file(\"./gpt2-song/config.json\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a00aKQtdta6M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_lyrics(prompt, max_length=150, temperature=0.9, top_p=0.95):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate lyrics with sampling\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=True,  # Enable sampling\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    # Decode the generated text\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
      ],
      "metadata": {
        "id": "PpNJnLdE4xoI"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Empty house, white noise\"\n",
        "lyrics = generate_lyrics(prompt)\n",
        "print(lyrics)"
      ],
      "metadata": {
        "id": "8bkf0jMQ4zjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "138f9457-c73b-4eb6-c743-7f025a847bf9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty house, white noise, and constant traffic. I've noticed other people's children have been trying to come in and play at the home with me, and they never came to my doorstep. I know that the police are still trying to find out what the house is.\n",
            "\n",
            "\"The problem is that they have no idea where it is, and they don't know how to find it. So there's only a handful of people who can help. I know that I'm not alone with the kids. I've got family members who help me, who also help us if we need them. So I can't understand how they don't understand, but the police don't understand. When I think about the police, that's all they\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tL6_UugevyjL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}